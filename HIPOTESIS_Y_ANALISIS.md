# Hip√≥tesis y Preguntas de Investigaci√≥n - Pr√°ctica 02

## Modelos Recurrentes para An√°lisis de Se√±ales de Motor

---

## üéØ Objetivos de Aprendizaje

1. Implementar y entrenar RNN, LSTM y GRU para clasificaci√≥n y regresi√≥n de se√±ales
2. Aplicar t√©cnicas de preprocesamiento de se√±ales para RNNs (ventanas, normalizaci√≥n)
3. Dise√±ar y evaluar variantes arquitecturales (bidireccionalidad, apilamiento, dropout)
4. Analizar m√©tricas (Accuracy, F1-Score, RMSE, R¬≤) y curvas de entrenamiento

---

## ‚ùì Preguntas de Investigaci√≥n

### Pregunta 1: Comparaci√≥n RNN vs LSTM/GRU
**¬øSuperar√° LSTM a la RNN simple en la captura de dependencias largas de las se√±ales de motor?**

- **Contexto:** Las RNN simples sufren del problema del gradiente desvaneciente en secuencias largas
- **Esperado:** LSTM y GRU deber√≠an superar a RNN en F1-Score y Accuracy
- **M√©tricas a observar:** 
  - Convergencia durante entrenamiento
  - F1-Score final en test
  - An√°lisis de matriz de confusi√≥n

### Pregunta 2: Impacto de Bidireccionalidad
**¬øMejorar√° la bidireccionalidad el F1-Score en la detecci√≥n de fallas incipientes?**

- **Contexto:** La bidireccionalidad permite capturar contexto pasado y futuro
- **Esperado:** Mejora en detecci√≥n de patrones sutiles en fallas tempranas
- **M√©tricas a observar:**
  - F1-Score por clase (especialmente fallas leves)
  - An√°lisis de falsos negativos en fallas incipientes

### Pregunta 3: Eficiencia GRU vs LSTM
**¬øLograr√° GRU un balance √≥ptimo entre rendimiento y eficiencia computacional comparado con LSTM?**

- **Contexto:** GRU tiene menos par√°metros que LSTM pero rendimiento similar
- **Esperado:** GRU comparable en m√©tricas pero m√°s r√°pido
- **M√©tricas a observar:**
  - F1-Score / RMSE comparables
  - Tiempo por √©poca
  - N√∫mero de par√°metros

### Pregunta 4: Profundidad vs Costo
**¬øLas variantes con mayor profundidad mejorar√°n las m√©tricas a costa de tiempo de entrenamiento?**

- **Contexto:** M√°s capas = m√°s capacidad pero mayor costo
- **Esperado:** Mejora marginal en m√©tricas con aumento significativo en tiempo
- **M√©tricas a observar:**
  - Ganancia en F1-Score / RMSE
  - Incremento en tiempo de entrenamiento
  - An√°lisis costo-beneficio

### Pregunta 5: Robustez ante Overfitting
**¬øQu√© arquitectura es m√°s robusta ante el overfitting en se√±ales de motor?**

- **Contexto:** Dropout y regularizaci√≥n ayudan a generalizar
- **Esperado:** Variantes con dropout mostrar√°n menor brecha train-val
- **M√©tricas a observar:**
  - Gap entre train y validation accuracy/loss
  - Rendimiento en test
  - Curvas de entrenamiento

---

## üí° Hip√≥tesis por Modelo

### RNN (Elman/Vanilla)

#### Modelo Base
**Hip√≥tesis:** *La RNN simple con activaci√≥n tanh capturar√° patrones b√°sicos temporales en las se√±ales de motor, pero mostrar√° limitaciones en dependencias largas.*

**Justificaci√≥n:**
- Tanh es est√°ndar para RNN
- Suficiente para patrones locales
- Problema de gradiente desvaneciente en secuencias largas

**Predicci√≥n:**
- Accuracy: 60-75%
- F1-Score: 55-70%
- Convergencia lenta despu√©s de √©poca 30

#### Variante 1: RNN Profunda (2 capas)
**Hip√≥tesis:** *Incrementar la profundidad (2 capas) mejorar√° la capacidad de modelar dependencias temporales complejas, aumentando F1-Score en 5-10% respecto al modelo base.*

**Justificaci√≥n:**
- Mayor capacidad de abstracci√≥n jer√°rquica
- Dos capas pueden capturar patrones en m√∫ltiples escalas temporales

**Predicci√≥n:**
- F1-Score: +5-10% vs base
- Tiempo por √©poca: +30-50%
- Posible overfitting si no se regulariza

#### Variante 2: RNN con ReLU
**Hip√≥tesis:** *Usar activaci√≥n ReLU en lugar de tanh evitar√° el problema del gradiente desvaneciente, mejorando la convergencia y m√©tricas finales.*

**Justificaci√≥n:**
- ReLU no sufre saturaci√≥n como tanh
- Gradientes m√°s estables en secuencias largas

**Predicci√≥n:**
- Convergencia m√°s r√°pida (alcanzar plateau en √©poca 40 vs 50)
- F1-Score similar o +3-5% vs tanh
- Posible "dying ReLU" si learning rate es muy alto

---

### LSTM (Long Short-Term Memory)

#### Modelo Base
**Hip√≥tesis:** *LSTM capturar√° dependencias a largo plazo mejor que RNN simple gracias a su mecanismo de compuertas (gates), super√°ndola en 10-15% en F1-Score.*

**Justificaci√≥n:**
- Gates (forget, input, output) previenen gradiente desvaneciente
- Cell state permite memoria a largo plazo
- Arquitectura probada en secuencias temporales

**Predicci√≥n:**
- Accuracy: 75-85%
- F1-Score: 70-80%
- Mejor detecci√≥n de patrones complejos

#### Variante 1: LSTM Bidireccional
**Hip√≥tesis:** *La bidireccionalidad permitir√° al modelo capturar contexto futuro y pasado simult√°neamente, mejorando F1-Score especialmente en fallas incipientes (+5-7% vs LSTM base).*

**Justificaci√≥n:**
- Contexto bidireccional √∫til para detecci√≥n de anomal√≠as
- Fallas incipientes pueden tener precursores y consecuencias

**Predicci√≥n:**
- F1-Score: +5-7% vs LSTM base
- Mejor performance en clases con fallas leves
- Par√°metros x2, tiempo por √©poca +40-60%

#### Variante 2: LSTM Apilada con Dropout
**Hip√≥tesis:** *Apilar capas LSTM con dropout (0.2) regularizar√° el modelo y mejorar√° la generalizaci√≥n en el conjunto de test, reduciendo el gap train-val en 3-5%.*

**Justificaci√≥n:**
- Dropout previene co-adaptaci√≥n de neuronas
- 2 capas aumentan capacidad de modelado
- Regularizaci√≥n mejora generalizaci√≥n

**Predicci√≥n:**
- Gap train-val: -3-5%
- F1-Score test similar o +2-3% vs base
- Menor overfitting en curvas de entrenamiento

---

### GRU (Gated Recurrent Unit)

#### Modelo Base
**Hip√≥tesis:** *GRU lograr√° rendimiento similar a LSTM (diferencia < 2% en F1-Score) pero con menor costo computacional debido a menos par√°metros (~25% menos).*

**Justificaci√≥n:**
- GRU simplifica LSTM (2 gates vs 3)
- Performance similar en muchas tareas
- M√°s eficiente computacionalmente

**Predicci√≥n:**
- F1-Score: 68-78% (similar a LSTM ¬±2%)
- Par√°metros: ~25% menos que LSTM
- Tiempo por √©poca: -15-20% vs LSTM

#### Variante 1: GRU Bidireccional
**Hip√≥tesis:** *GRU bidireccional mejorar√° la detecci√≥n de patrones en ambas direcciones temporales, logrando F1-Score comparable a LSTM bidireccional pero con menor costo.*

**Justificaci√≥n:**
- Bidireccionalidad √∫til independiente de arquitectura
- GRU mantiene eficiencia incluso bidireccional

**Predicci√≥n:**
- F1-Score: dentro de ¬±2% de LSTM bidireccional
- Tiempo: -10-15% vs LSTM bidireccional
- Mejor costo-beneficio

#### Variante 2: GRU Apilada
**Hip√≥tesis:** *GRU apilada (2 capas con dropout) aumentar√° la capacidad de modelado manteniendo eficiencia computacional, logrando el mejor balance rendimiento-costo.*

**Justificaci√≥n:**
- 2 capas GRU < par√°metros que 2 capas LSTM
- Dropout regulariza
- Mantiene eficiencia de GRU

**Predicci√≥n:**
- F1-Score: +4-6% vs GRU base
- Tiempo: +25-35% vs GRU base (a√∫n < LSTM apilada)
- **Mejor candidato para producci√≥n** (balance √≥ptimo)

---

## üìä Tabla de Predicciones Esperadas

### Clasificaci√≥n

| Modelo | Accuracy (%) | F1-Score (%) | Params (M) | Tiempo/√âpoca (s) | Observaciones |
|--------|--------------|--------------|------------|------------------|---------------|
| RNN Base | 60-75 | 55-70 | ~0.15 | 2-3 | Baseline, gradiente desvaneciente |
| RNN Deep | 65-80 | 60-75 | ~0.25 | 3-4 | +5-10% F1, m√°s profundidad |
| RNN ReLU | 65-78 | 60-73 | ~0.15 | 2-3 | Mejor convergencia |
| LSTM Base | 75-85 | 70-80 | ~0.20 | 4-5 | Buenas dependencias largas |
| LSTM Bi | 78-88 | 75-85 | ~0.40 | 6-8 | +5-7% F1, contexto bidireccional |
| LSTM Stack | 76-87 | 72-82 | ~0.35 | 5-7 | Mejor generalizaci√≥n |
| GRU Base | 74-84 | 68-78 | ~0.15 | 3-4 | Similar a LSTM, m√°s eficiente |
| GRU Bi | 77-87 | 73-83 | ~0.30 | 5-6 | Comparable a LSTM Bi |
| GRU Stack | 76-86 | 72-82 | ~0.25 | 4-5 | **Mejor balance** |

### Regresi√≥n

| Modelo | RMSE | R¬≤ | Params (M) | Tiempo/√âpoca (s) | Observaciones |
|--------|------|-----|------------|------------------|---------------|
| RNN Base | 0.08-0.12 | 0.75-0.85 | ~0.10 | 2-3 | Baseline |
| RNN Deep | 0.06-0.10 | 0.80-0.90 | ~0.18 | 3-4 | Mejor que base |
| RNN ReLU | 0.07-0.11 | 0.78-0.88 | ~0.10 | 2-3 | Convergencia m√°s r√°pida |
| LSTM Base | 0.05-0.08 | 0.85-0.92 | ~0.15 | 3-4 | Buena predicci√≥n |
| LSTM Bi | 0.04-0.07 | 0.88-0.94 | ~0.30 | 5-6 | Mejor contexto |
| LSTM Stack | 0.04-0.07 | 0.87-0.93 | ~0.25 | 4-5 | Regularizado |
| GRU Base | 0.05-0.09 | 0.84-0.91 | ~0.12 | 3-4 | Eficiente |
| GRU Bi | 0.04-0.08 | 0.86-0.93 | ~0.24 | 4-5 | Balance bueno |
| GRU Stack | 0.04-0.08 | 0.86-0.92 | ~0.20 | 3-4 | **Mejor opci√≥n** |

---

## üîç Aspectos Clave a Analizar

### 1. Gradiente Desvaneciente en RNN
**¬øC√≥mo verificarlo?**
- Observar norma de gradientes durante entrenamiento
- Comparar convergencia RNN vs LSTM/GRU
- Analizar rendimiento en dependencias largas vs cortas

**Indicadores:**
- RNN plateau temprano en curvas
- LSTM/GRU contin√∫an mejorando
- Gap en F1-Score significativo

### 2. Impacto de Bidireccionalidad
**¬øD√≥nde se nota m√°s?**
- Matriz de confusi√≥n: mejor en clases dif√≠ciles
- F1-Score por clase
- An√°lisis de errores (falsos positivos/negativos)

**Esperado:**
- Mejora mayor en fallas leves/incipientes
- Reducci√≥n de confusi√≥n entre clases similares

### 3. Eficiencia Computacional
**M√©tricas:**
- Par√°metros totales
- Tiempo por √©poca
- Memoria GPU utilizada
- Ratio (F1-Score / Tiempo)

**Esperado:**
- GRU m√°s eficiente que LSTM
- Bidireccionalidad duplica tiempo ~50-70%
- Apilamiento aumenta tiempo ~30-50%

### 4. Overfitting
**Se√±ales:**
- Gap train-val loss > 0.2
- Accuracy train > val por >10%
- Curvas de val comenzando a divergir

**Soluciones implementadas:**
- Dropout en variantes apiladas
- Weight decay (1e-5)
- Early stopping (opcional)

---

## üìù Gu√≠a para el An√°lisis Final

### Secci√≥n de Resultados (III)
1. Presentar tablas con m√©tricas
2. Mostrar curvas de entrenamiento
3. Matrices de confusi√≥n de mejores modelos
4. Gr√°ficas de comparaci√≥n

### Secci√≥n de An√°lisis (IV)

#### 4.1 Validaci√≥n de Hip√≥tesis
Para cada hip√≥tesis:
- ‚úÖ **Confirmada:** Si predicci√≥n ¬±5% de realidad
- ‚ö†Ô∏è **Parcialmente confirmada:** Si predicci√≥n ¬±10%
- ‚ùå **Rechazada:** Si predicci√≥n >10% err√≥nea

Explicar **por qu√©** en cada caso.

#### 4.2 Comparaci√≥n RNN vs LSTM/GRU
Analizar:
- Diferencias en F1-Score
- Curvas de convergencia
- Problema de gradiente desvaneciente (evidencia)

#### 4.3 Comparaci√≥n LSTM vs GRU
Evaluar:
- Trade-off rendimiento-costo
- Casos donde uno supera al otro
- Recomendaci√≥n para producci√≥n

#### 4.4 An√°lisis de Errores
Clasificaci√≥n:
- Clases m√°s confundidas (matriz confusi√≥n)
- ¬øFallas leves con sano?
- ¬øConfusi√≥n entre niveles adyacentes?

Regresi√≥n:
- Scatter plot: ¬øsesgo en rangos?
- ¬øSubestimaci√≥n/sobreestimaci√≥n?
- Residuales: ¬øpatrones?

#### 4.5 Impacto de Variantes
Para cada variante:
- ¬øMejora significativa? (>3%)
- ¬øCosto computacional justificado?
- ¬øCu√°ndo usar cada una?

---

## üéì Lecciones Aprendidas Esperadas

1. **RNN simple:** Limitada para dependencias largas
2. **LSTM:** Excelente pero costosa
3. **GRU:** Mejor balance rendimiento-costo
4. **Bidireccionalidad:** √ötil cuando hay contexto futuro disponible
5. **Profundidad:** Rendimiento decreciente, regularizaci√≥n cr√≠tica
6. **Dropout:** Esencial para generalizaci√≥n
7. **Normalizaci√≥n:** Cr√≠tica para convergencia
8. **Ventanas:** Tama√±o importante (64 parece adecuado)

---

## üöÄ Pr√≥ximos Pasos (Opcional)

1. **Attention mechanisms:** LSTM/GRU con attention
2. **Ensemble:** Combinar predicciones de m√∫ltiples modelos
3. **Arquitecturas h√≠bridas:** CNN + LSTM para se√±ales
4. **Transfer learning:** Pre-entrenar en se√±ales similares
5. **Optimizaci√≥n:** B√∫squeda de hiperpar√°metros autom√°tica

---

**Autor:** Implementaci√≥n acad√©mica para Aprendizaje Profundo  
**Fecha:** Noviembre 2025
